{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-14T06:29:32.015295Z",
     "start_time": "2023-09-14T06:29:31.974058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature = 0, model=\"gpt-3.5-turbo\")\n",
    "message = \"\"\"\n",
    "In ChatOpenAI, there are three types of messages: HumanMessage, SystemMessage, and AIMessage.\n",
    "\n",
    "HumanMessage is a message sent from the perspective of the human. It can be anything that the human wants to say to the AI, such as a question, a request, or a statement.\n",
    "SystemMessage is a message setting the objectives the AI should follow. It is sent from the system to the AI, and it can be used to give the AI instructions or to provide feedback.\n",
    "AIMessage is a message sent from the perspective of the AI. It can be anything that the AI wants to say to the human, such as an answer, a suggestion, or a clarification.\n",
    "The other one you mentioned is ChatMessage, which is a message allowing for arbitrary setting of role. It is a subclass of HumanMessage and AIMessage.\"\"\"\n",
    "text = [\n",
    "    SystemMessage(content=\"You are an expert copy writer with expertise in summarizing content\"), \n",
    "    HumanMessage(content=f\"Please provide a short and concise summary of the following,  Text:{message}\")\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T06:51:18.975117Z",
     "start_time": "2023-09-14T06:51:18.962136Z"
    }
   },
   "id": "ef4e41a5b710d3d7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "364"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T06:36:08.609907Z",
     "start_time": "2023-09-14T06:36:08.557797Z"
    }
   },
   "id": "f004d3f97a6060d0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "summary = llm(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T06:36:44.852858Z",
     "start_time": "2023-09-14T06:36:37.579992Z"
    }
   },
   "id": "24bfc664648a1673"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'The temperature in ChatOpenAI controls the randomness of generated text. A higher temperature leads to more creative and diverse text, while a lower temperature results in more conservative and predictable text. The temperature value ranges from 0 to 1, with 0 always choosing the most likely word and 1 choosing any word with equal probability. For creative tasks, a temperature of 0.7 or higher is recommended, while for factual tasks, a temperature of 0.4 or lower is preferred. For a mix of creativity and factuality, a temperature of 0.5 or 0.6 can be used. Experimentation with different temperature settings is encouraged to find the best fit.'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T06:37:10.056975Z",
     "start_time": "2023-09-14T06:37:09.983168Z"
    }
   },
   "id": "254d9cf950f621ff"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T06:43:30.463914Z",
     "start_time": "2023-09-14T06:43:30.421455Z"
    }
   },
   "id": "46bde6d6e8c36d4f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'ChatOpenAI में तीन प्रकार के संदेश होते हैं: HumanMessage, SystemMessage और AIMessage. HumanMessage मानव के दृष्टिकोण से भेजा गया संदेश होता है। यह कुछ भी हो सकता है जो मानव एआई को कहना चाहता है, जैसे सवाल, अनुरोध या कथन। SystemMessage एआई को अनुसरण करने के लिए उद्देश्य सेट करने वाला संदेश होता है। यह सिस्टम से एआई को निर्देश देने या प्रतिक्रिया प्रदान करने के लिए उपयोग किया जा सकता है। AIMessage एआई के दृष्टिकोण से भेजा गया संदेश होता है। यह कुछ भी हो सकता है जो एआई मानव को कहना चाहता है, जैसे जवाब, सुझाव या स्पष्टीकरण। आपने उल्लेख किया है वह एक और है ChatMessage, जो भूमिका की विचारशील सेटिंग की अनुमति देता है। यह HumanMessage और AIMessage का एक उपप्रकार है।'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = (\"Write a short and concise summary of the following text\"\n",
    "            \"Text: {text}. Translate the summary to {language}\")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"text\", \"language\"],\n",
    "    template=template\n",
    ")\n",
    "chain = LLMChain(llm= llm, prompt=prompt)\n",
    "summary = chain.run({\"text\": message, 'language': \"hindi\"})\n",
    "summary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T06:53:46.430214Z",
     "start_time": "2023-09-14T06:53:21.102605Z"
    }
   },
   "id": "f7164d98685814dd"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'The speaker, who never graduated from college, shares three stories from his life during a commencement speech. The first story is about the importance of connecting the dots and how dropping out of college led him to take a calligraphy class that later influenced the design of the Macintosh computer. The second story is about love and loss, as he reflects on being fired from the company he co-founded and how it ultimately led to new opportunities and personal growth. The third story is about death, as he recounts his experience with a cancer diagnosis and emphasizes the importance of living life to the fullest. The speaker encourages the graduates to follow their passions, trust their intuition, and stay hungry and foolish in their pursuit of success.'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open('../files/sj.txt', encoding='utf-8') as f:\n",
    "    sj_message = f.read()\n",
    "docs = [Document(page_content= sj_message)]\n",
    "template = (\"Write a short and concise summary of the following text\"\n",
    "            \"Text: {text}. \")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"text\"],\n",
    "    template=template\n",
    ")\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type = 'stuff',\n",
    "    prompt = prompt,\n",
    "    verbose=False\n",
    ")\n",
    "chain.run(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T07:10:06.118958Z",
     "start_time": "2023-09-14T07:09:58.544914Z"
    }
   },
   "id": "a8993af8516b084b"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Steve Jobs shares three stories from his life, including dropping out of college and how it influenced the design of the Macintosh computer, getting fired from Apple and finding success in new ventures, and his experience with cancer and the importance of living each day fully. He emphasizes the importance of following one's passion, not settling, and embracing the inevitability of death. Jobs encourages the audience to live their own lives, follow their hearts, and mentions The Whole Earth Catalog as an inspiration. He concludes by wishing the audience well on their new beginnings.\""
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open('../files/sj.txt', encoding='utf-8') as f:\n",
    "    sj_message = f.read()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50)\n",
    "# docs = [Document(page_content= sj_message)]\n",
    "chunks = splitter.create_documents([sj_message])\n",
    "template = (\"Write a short and concise summary of the following text\"\n",
    "            \"Text: {text}. \")\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables = [\"text\"],\n",
    "#     template=template\n",
    "# )\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type = 'map_reduce',\n",
    "    verbose=False\n",
    ")\n",
    "chain.run(chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T07:34:15.574206Z",
     "start_time": "2023-09-14T07:33:58.516898Z"
    }
   },
   "id": "75b151d298a051c"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " chain.llm_chain.prompt.template\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T07:36:57.416008Z",
     "start_time": "2023-09-14T07:36:57.396222Z"
    }
   },
   "id": "c5699adcc7f3bb62"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.combine_document_chain.llm_chain.prompt.template"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T07:37:04.890476Z",
     "start_time": "2023-09-14T07:37:04.857457Z"
    }
   },
   "id": "7fed3eb715278157"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "loader = UnstructuredPDFLoader(\"../files/attention_is_all_you_need.pdf\")\n",
    "kenya_cons = loader.load()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:34:02.851668Z",
     "start_time": "2023-09-14T09:33:59.130049Z"
    }
   },
   "id": "59986bc74e8541da"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9762\n",
      "9993\n",
      "9801\n",
      "9994\n",
      "466\n",
      "Total tokens: 9995\n",
      "Embedding cost in USD: 0.003998\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=5)\n",
    "chunks = splitter.split_documents(kenya_cons)\n",
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total tokens: {total_tokens}')\n",
    "    print(f'Embedding cost in USD: {total_tokens / 1000 * 0.0004:.6f}')\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(len(chunk.page_content))\n",
    "print_embedding_cost(chunks)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:38:01.385993Z",
     "start_time": "2023-09-14T09:38:01.350457Z"
    }
   },
   "id": "f84793bf2cc979eb"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='refine',\n",
    "    verbose=False\n",
    ")\n",
    "summary = chain.run(chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:38:58.348823Z",
     "start_time": "2023-09-14T09:38:10.542845Z"
    }
   },
   "id": "5c595d1adc0e7cca"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "'The paper introduces the Transformer, a network architecture based solely on attention mechanisms, which achieves superior results in machine translation tasks compared to existing models. The Transformer is more parallelizable and requires less training time. The paper discusses the advantages of self-attention and describes the architecture of the Transformer, including the encoder and decoder stacks and the attention mechanism used. The paper also compares self-attention to recurrent and convolutional layers in terms of computational complexity, parallelizability, and the ability to learn long-range dependencies. The authors propose using positional encodings to inject information about the relative or absolute position of tokens in the sequence. The paper concludes by discussing the training process and the potential interpretability of self-attention models. Additionally, the paper provides details on the training data, hardware, schedule, optimizer, and regularization techniques used. It also presents results on machine translation tasks and English constituency parsing, demonstrating the effectiveness and generalizability of the Transformer model. The authors express excitement about the future of attention-based models and their plans to apply them to other tasks, such as handling input and output modalities other than text and making generation less sequential. The code used to train and evaluate the models is available on GitHub. The authors acknowledge the contributions of Nal Kalchbrenner and Stephan Gouws. The paper also highlights that many of the attention heads in the Transformer exhibit behavior related to the structure of the sentence, indicating that they have learned to perform different tasks.'"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:39:10.313954Z",
     "start_time": "2023-09-14T09:39:10.285160Z"
    }
   },
   "id": "a2e286daf4f7be54"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "map_prompt = '''\n",
    "Write a short and concise summary of the following:\n",
    "Text: `{text}`\n",
    "CONCISE SUMMARY:\n",
    "'''\n",
    "map_prompt_template = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=map_prompt\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:41:54.174096Z",
     "start_time": "2023-09-14T09:41:54.140693Z"
    }
   },
   "id": "86c81e506777e018"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "combine_prompt = '''\n",
    "Write a concise summary of the following text that covers the key points.\n",
    "Add a title to the summary.\n",
    "Start your summary with an INTRODUCTION PARAGRAPH that gives an overview of the topic FOLLOWED\n",
    "by BULLET POINTS if possible AND end the summary with a CONCLUSION PHRASE.\n",
    "Text: `{text}`\n",
    "'''\n",
    "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:42:25.237576Z",
     "start_time": "2023-09-14T09:42:25.203955Z"
    }
   },
   "id": "2f7dd9e114cff097"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Title: The Transformer: A New Network Architecture Based on Attention Mechanisms\\n\\nIntroduction:\\nThe paper introduces the Transformer, a network architecture that relies solely on attention mechanisms and eliminates the need for recurrent or convolutional neural networks. The Transformer achieves superior results in machine translation tasks, offers parallelizability, and requires less training time compared to existing models.\\n\\nKey Points:\\n- The Transformer model utilizes self-attention and fully connected layers in both the encoder and decoder.\\n- The attention mechanism used in the Transformer is called Scaled Dot-Product Attention.\\n- The text compares additive attention and dot-product attention, with the latter being faster and more space-efficient.\\n- Multi-head attention allows the model to attend to different information simultaneously.\\n- Self-attention layers and positional encodings capture sequence order and outperform recurrent and convolutional layers in terms of computational complexity and path length.\\n- The training regime for the Transformer model involves large datasets, byte-pair encoding, batched sequences, and the Adam optimizer.\\n- Regularization techniques like residual dropout and label smoothing are employed during training.\\n- The Transformer model surpasses previous state-of-the-art models in translation quality and training cost, and shows promise in English constituency parsing.\\n- Future plans include extending the Transformer to handle inputs and outputs beyond text, such as images, audio, and video.\\n- The code used for training and evaluation is available on GitHub, and the authors acknowledge the contributions of others in the field.\\n\\nConclusion:\\nThe Transformer introduces a novel network architecture based on attention mechanisms, achieving remarkable results in machine translation tasks while offering advantages in parallelizability and training time. The model's potential for handling various types of inputs and outputs opens up new possibilities for generation.\""
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=map_prompt_template,\n",
    "    combine_prompt=combine_prompt_template,\n",
    "    verbose=False\n",
    ")\n",
    "summary_chain.run(chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:44:04.734965Z",
     "start_time": "2023-09-14T09:43:23.821169Z"
    }
   },
   "id": "cad5a82c7bd4445f"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following extracting the key information:\n",
    "Text: `{text}`\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "initial_prompt = PromptTemplate(template=prompt_template, input_variables=['text'])\n",
    "\n",
    "refine_template = '''\n",
    "    Your job is to produce a final summary.\n",
    "    I have provided an existing summary up to a certain point: {existing_answer}.\n",
    "    Please refine the existing summary with some more context below.\n",
    "    ------------\n",
    "    {text}\n",
    "    ------------\n",
    "    Start the final summary with an INTRODUCTION PARAGRAPH that gives an overview of the topic FOLLOWED\n",
    "    by BULLET POINTS if possible AND end the summary with a CONCLUSION PHRASE.\n",
    "    \n",
    "'''\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_template,\n",
    "    input_variables=['existing_answer', 'text']\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:46:30.628258Z",
     "start_time": "2023-09-14T09:46:30.591314Z"
    }
   },
   "id": "5e12f77f8b045a94"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    question_prompt=initial_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=False\n",
    "    \n",
    ")\n",
    "output_summary = chain.run(chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:47:52.768912Z",
     "start_time": "2023-09-14T09:46:34.567826Z"
    }
   },
   "id": "87c537028e67a6f2"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "\"The paper introduces the Transformer, a network architecture that relies solely on attention mechanisms and eliminates the need for recurrent or convolutional neural networks. The Transformer achieves superior results in machine translation tasks, offers better parallelization capabilities, and requires less training time compared to existing models. It also demonstrates state-of-the-art performance in translation quality and generalizes well to other tasks. The paper provides a detailed description of the Transformer's architecture, including the encoder and decoder stacks, as well as the attention mechanism used. The training regime for the models is also discussed, including the training data, hardware and schedule, optimizer, and regularization techniques. The Transformer architecture, based solely on attention mechanisms, offers significant advantages in terms of performance, parallelization, and training time compared to existing models. By employing multi-head attention and position-wise feed-forward networks, the Transformer achieves state-of-the-art results in translation quality and demonstrates the ability to generalize well to other tasks. The paper's detailed description of the Transformer's architecture and attention mechanism provides valuable insights into the inner workings of this innovative network model.\\n\\nIn addition, the authors express their excitement about the future of attention-based models and their plans to apply them to other tasks. They also mention their intention to extend the Transformer to handle input and output modalities other than text, such as images, audio, and video. Furthermore, they express their interest in exploring local, restricted attention mechanisms to efficiently handle large inputs and outputs. Another research goal mentioned is making generation less sequential.\\n\\nThe paper also provides a link to the code used to train and evaluate the models, which is available on GitHub. The authors acknowledge the contributions of Nal Kalchbrenner and Stephan Gouws for their comments, corrections, and inspiration.\\n\\nIn conclusion, the Transformer architecture, based solely on attention mechanisms, offers significant advantages in terms of performance, parallelization, and training time compared to existing models. The paper provides a detailed description of the Transformer's architecture and attention mechanism, as well as insights into its inner workings. The authors express their excitement about the future of attention-based models and their plans to apply them to other tasks.\""
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_summary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:48:04.160814Z",
     "start_time": "2023-09-14T09:48:04.131346Z"
    }
   },
   "id": "7729793ffdae2f4d"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Wikipedia\", \n",
    "        func=wikipedia.run,\n",
    "        description=\"Useful for when you need to get information from wikipedia about a single topic\"\n",
    "    )\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:49:07.809136Z",
     "start_time": "2023-09-14T09:49:07.503339Z"
    }
   },
   "id": "a6ea3b131052ffed"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "agent_executor = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:49:23.029244Z",
     "start_time": "2023-09-14T09:49:22.982862Z"
    }
   },
   "id": "adb5d6da1155ae6"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mI should use Wikipedia to find information about the state of taxes in Kenya.\n",
      "Action: Wikipedia\n",
      "Action Input: \"Taxes in Kenya\"\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3mPage: Kenya Revenue Authority\n",
      "Summary: Kenya Revenue Authority (KRA), is an agency of the Government of Kenya that is responsible for the assessment, collection and accounting of all revenues that are due to the government in accordance with the laws of Kenya.\n",
      "\n",
      "Page: Kenya\n",
      "Summary: Kenya, officially the Republic of Kenya (Swahili: Jamhuri ya Kenya), is a country in East Africa. A member of the Commonwealth with a population of more than 47.6 million in the 2019 census, Kenya is the 28th most populous country in the world and 7th most populous in Africa. Kenya's capital and largest city is Nairobi, while its oldest and second largest city, which until 1907 was also Kenya's first capital city, is the coastal city of Mombasa which includes Mombasa Island in the Indian Ocean and the surrounding mainland. Kisumu is the third-largest city and also an inland port in the Winam Gulf which, along with its numerous bays and human settlements, is one of the important maritime transport, fishing, farming, commercial, history and tourism hubs on Lake Victoria. As of 2020, Kenya is the third-largest economy in sub-Saharan Africa after Nigeria and South Africa. Kenya is bordered by South Sudan to the northwest, Ethiopia to the north, Somalia to the east, Uganda to the west, Tanzania to the south, and the Indian Ocean to the southeast. Its geography, climate and population vary widely, ranging from cold snow-capped mountaintops (Batian, Nelion and Point Lenana on Mount Kenya) with vast surrounding forests, wildlife and fertile agricultural regions to temperate climates in western and rift valley counties and dry less fertile arid and semi-arid areas and absolute deserts (Chalbi Desert and Nyiri Desert).\n",
      "Kenya's earliest inhabitants were hunter-gatherers, like the present-day Hadza people. According to archaeological dating of associated artifacts and skeletal material, Cushitic speakers first settled in Kenya's lowlands between 3,200 and 1,300 BC, a phase known as the Lowland Savanna Pastoral Neolithic. Nilotic-speaking pastoralists (ancestral to Kenya's Nilotic speakers) began migrating from present-day South Sudan into Kenya around 500 BC. Bantu people settled at the coast and the interior between 250 BC and 500 AD. European contact began in 1500 AD with the Portuguese Empire, and effective colonisation of Kenya began in the 19th century during the European exploration of the interior. Modern-day Kenya emerged from a protectorate established by the British Empire in 1895 and the subsequent Kenya Colony, which began in 1920. Numerous disputes between the UK and the colony led to the Mau Mau revolution, which began in 1952, and the declaration of independence in 1963. After independence, Kenya remained a member of the Commonwealth of Nations. The current constitution was adopted in 2010 and replaced the 1963 independence constitution.\n",
      "Kenya is a presidential representative democratic republic, in which elected officials represent the people and the president is the head of state and government. Kenya is a member of the United Nations, Commonwealth of Nations, World Bank, International Monetary Fund, COMESA, International Criminal Court, as well as other international organisations. With a GNI of 1,840, Kenya is a lower-middle-income economy. Kenya's economy is the largest in eastern and central Africa, with Nairobi serving as a major regional commercial hub. Agriculture is the largest sector: tea and coffee are traditional cash crops, while fresh flowers are a fast-growing export. The service industry is also a major economic driver, particularly tourism. Kenya is a member of the East African Community trade bloc, though some international trade organisations categorise it as part of the Greater Horn of Africa. Africa is Kenya's largest export market, followed by the European Union.\n",
      "\n",
      "Page: Kenyan taxation system\n",
      "Summary: Kenya's taxation system covers income tax, value-added tax, customs and excise duty. The regulations are governed by indepen\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3mI now know the final answer.\n",
      "Final Answer: The state of taxes in Kenya is governed by the Kenya Revenue Authority (KRA), which is responsible for the assessment, collection, and accounting of all revenues due to the government. The taxation system in Kenya covers income tax, value-added tax, customs, and excise duty.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'The state of taxes in Kenya is governed by the Kenya Revenue Authority (KRA), which is responsible for the assessment, collection, and accounting of all revenues due to the government. The taxation system in Kenya covers income tax, value-added tax, customs, and excise duty.'"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run('Can you please provide a short summary of the state of taxes currently in kenya')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T09:51:21.355330Z",
     "start_time": "2023-09-14T09:51:08.546992Z"
    }
   },
   "id": "564025e5ec6747b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9692aa6fd488ca4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
